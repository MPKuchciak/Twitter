{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to download tweets from the X API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download all the data used in our research, create an account at https://developer.x.com and purchase access to the X API. You will need an account with a pull limit of at least 50,000 tweets (4 Basic accounts for $200/month each or 1 Pro account for $5,000/month).  \n",
    "Then, run all the code snippets below and follow the instructions.\n",
    "\n",
    "**Note:** You must provide your own X API credentials and ensure your account has sufficient access (at least 50,000 tweets).  \n",
    "- Use section **3.1** to download tweets for a single user.  \n",
    "- Use section **3.2** to automatically download tweets for all users included in the study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os \n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.helpers_downloader import authenticate_app_only, fetch_all_tweet_fields, save_tweets_to_json, download_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bear token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Bearer Token is required.\n"
     ]
    }
   ],
   "source": [
    "# Provide your Bearer Token here for Twitter API access\n",
    "BEARER_TOKEN = input(\"Please enter your Bearer Token: \") \n",
    "\n",
    "if not BEARER_TOKEN:\n",
    "    print(\"Error: Bearer Token is required.\")\n",
    "\n",
    "client = authenticate_app_only(BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Final downloading loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the final implementation for the Tweepy paginator. It does not resolve all issues, which is why we had to implement a custom while loop. In this loop, we check the date of the last downloaded tweet and request tweets between the `START_TIME` and the date of the last tweet ( + 1 second) to ensure we have downloaded all tweets from the specified time range. When some users post and retweet a lot, the API does not work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Single person downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"mwojcik_\" #To change: Twitter username\n",
    "EXCLUDE_RETWEETS = True\n",
    "EXCLUDE_REPLIES = False\n",
    "\n",
    "START_TIME = \"2023-10-16T00:00:00Z\" #To change: Start time\n",
    "END_TIME = \"2024-10-15T23:59:59Z\" #To change: End time\n",
    "\n",
    "org_START_TIME = START_TIME\n",
    "org_END_TIME = END_TIME\n",
    "\n",
    "tweets = []\n",
    "\n",
    "new_tweets = fetch_all_tweet_fields(\n",
    "        client=client,\n",
    "        username=username,\n",
    "        start_time=START_TIME,\n",
    "        end_time=END_TIME,\n",
    "        exclude_retweets=EXCLUDE_RETWEETS,\n",
    "        exclude_replies=EXCLUDE_REPLIES\n",
    "    )\n",
    "tweets.extend(new_tweets)\n",
    "print(f'Fetched {len(new_tweets)} tweets')\n",
    "\n",
    "while (len(new_tweets) > 1):\n",
    "    last_tweet_time = datetime.fromisoformat(tweets[-1][\"created_at\"].replace(\"Z\", \"+00:00\"))\n",
    "    new_time = last_tweet_time + timedelta(seconds=1)\n",
    "    new_time_iso = new_time.isoformat().replace(\"+00:00\", \"Z\")\n",
    "    END_TIME = new_time_iso\n",
    "    print(f\"Fetching tweets from {START_TIME} to {END_TIME}\")\n",
    "    new_tweets = fetch_all_tweet_fields(\n",
    "        client=client,\n",
    "        username=username,\n",
    "        start_time=START_TIME,\n",
    "        end_time=END_TIME,\n",
    "        exclude_retweets=EXCLUDE_RETWEETS,\n",
    "        exclude_replies=EXCLUDE_REPLIES\n",
    "    )\n",
    "    print(f'Fetched {len(new_tweets)} tweets')\n",
    "    tweets.extend(new_tweets)    \n",
    "    sleep(10)\n",
    "\n",
    "save_tweets_to_json(tweets, f\"{username}_{org_START_TIME[0:10]}_{org_END_TIME[0:10]}.json\",folder=\"../data/01.raw/tweets_before_elections/PiS\")  #To change: Folder !IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Research reproducibility: downloading all data used in the study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading tweets for all users before the elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bartlomiejpejo from Konfederacja, already downloaded.\n",
      "Skipping GrzegorzBraun_ from Konfederacja, already downloaded.\n",
      "Skipping JkmMikke from Konfederacja, already downloaded.\n",
      "Skipping KonradBerkowicz from Konfederacja, already downloaded.\n",
      "Skipping krzysztofbosak from Konfederacja, already downloaded.\n",
      "Skipping MarSypniewski from Konfederacja, already downloaded.\n",
      "Skipping MichalWawer from Konfederacja, already downloaded.\n",
      "Skipping placzekgrzegorz from Konfederacja, already downloaded.\n",
      "Skipping RJ_Iwaszkiewicz from Konfederacja, already downloaded.\n",
      "Skipping SlawomirMentzen from Konfederacja, already downloaded.\n",
      "Skipping TudujKrzysztof from Konfederacja, already downloaded.\n",
      "Skipping Wlodek_Skalik from Konfederacja, already downloaded.\n",
      "Skipping WTumanowicz from Konfederacja, already downloaded.\n",
      "Skipping AndrzejSzejna from NL, already downloaded.\n",
      "Skipping AnitaKDZG from NL, already downloaded.\n",
      "Skipping Arek_Iwaniak from NL, already downloaded.\n",
      "Skipping B_Maciejewska from NL, already downloaded.\n",
      "Skipping JoankaSW from NL, already downloaded.\n",
      "Skipping KGawkowski from NL, already downloaded.\n",
      "Skipping K_Smiszek from NL, already downloaded.\n",
      "Skipping MarcinKulasek from NL, already downloaded.\n",
      "Skipping MoskwaWodnicka from NL, already downloaded.\n",
      "Skipping PaulinaPW2024 from NL, already downloaded.\n",
      "Skipping RobertBiedron from NL, already downloaded.\n",
      "Skipping WandaNowicka from NL, already downloaded.\n",
      "Skipping wieczorekdarek from NL, already downloaded.\n",
      "Skipping wlodekczarzasty from NL, already downloaded.\n",
      "Skipping AC_Sobol from PiS, already downloaded.\n",
      "Skipping BeataSzydlo from PiS, already downloaded.\n",
      "Skipping elzbietawitek from PiS, already downloaded.\n",
      "Skipping jbrudzinski from PiS, already downloaded.\n",
      "Skipping Kaminski_M_ from PiS, already downloaded.\n",
      "Skipping Kowalczyk_H from PiS, already downloaded.\n",
      "Skipping Macierewicz_A from PiS, already downloaded.\n",
      "Skipping mblaszczak from PiS, already downloaded.\n",
      "Skipping MorawieckiM from PiS, already downloaded.\n",
      "Skipping mwojcik_ from PiS, already downloaded.\n",
      "Skipping PatrykJaki from PiS, already downloaded.\n",
      "Skipping PMilowanski from PiS, already downloaded.\n",
      "Skipping AgaBaranowskaPL from PL2050, already downloaded.\n",
      "Skipping aga_buczynska from PL2050, already downloaded.\n",
      "Skipping hennigkloska from PL2050, already downloaded.\n",
      "Skipping JKozlowskiEu from PL2050, already downloaded.\n",
      "Skipping joannamucha from PL2050, already downloaded.\n",
      "Skipping Kpelczynska from PL2050, already downloaded.\n",
      "Skipping LukaszOsmalak from PL2050, already downloaded.\n",
      "Skipping michalkobosko from PL2050, already downloaded.\n",
      "Skipping SlizPawel from PL2050, already downloaded.\n",
      "Skipping szymon_holownia from PL2050, already downloaded.\n",
      "Skipping ZalewskiPawel from PL2050, already downloaded.\n",
      "Skipping ZywnoMaciej from PL2050, already downloaded.\n",
      "Skipping Arlukowicz from PO, already downloaded.\n",
      "Skipping bbudka from PO, already downloaded.\n",
      "Skipping CTomczyk from PO, already downloaded.\n",
      "Skipping donaldtusk from PO, already downloaded.\n",
      "Skipping DorotaNiedziela from PO, already downloaded.\n",
      "Skipping EwaKopacz from PO, already downloaded.\n",
      "Skipping JanGrabiec from PO, already downloaded.\n",
      "Skipping Konwinski_PO from PO, already downloaded.\n",
      "Skipping Leszczyna from PO, already downloaded.\n",
      "Skipping MKierwinski from PO, already downloaded.\n",
      "Skipping M_K_Blonska from PO, already downloaded.\n",
      "Skipping OklaDrewnowicz from PO, already downloaded.\n",
      "Skipping TomaszSiemoniak from PO, already downloaded.\n",
      "Skipping trzaskowski_ from PO, already downloaded.\n",
      "Skipping DariuszKlimczak from PSL, already downloaded.\n",
      "Skipping GrzybAndrzej from PSL, already downloaded.\n",
      "Skipping Hetman_K from PSL, already downloaded.\n",
      "Skipping JarubasAdam from PSL, already downloaded.\n",
      "Skipping KosiniakKamysz from PSL, already downloaded.\n",
      "Skipping Paslawska from PSL, already downloaded.\n",
      "Skipping PZgorzelskiP from PSL, already downloaded.\n",
      "Skipping StefanKrajewski from PSL, already downloaded.\n",
      "Skipping StruzikAdam from PSL, already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Read the people to download \n",
    "with open('../data/00.init/people_before_elections.json', 'r', encoding='utf-8') as f:\n",
    "    people_before_elections = json.load(f)\n",
    "\n",
    "# Check people which are already downloaded\n",
    "tweets_before_elections_path = \"../data/01.raw/tweets_before_elections\"\n",
    "all_files_before_elections = os.listdir(tweets_before_elections_path)\n",
    "all_files_dict = {}\n",
    "for party in all_files_before_elections:\n",
    "    party_folder = os.path.join(tweets_before_elections_path, party)\n",
    "    all_files_dict[party] = os.listdir(party_folder)\n",
    "all_downloaded_usernames = {}\n",
    "for party, files in all_files_dict.items():\n",
    "    all_downloaded_usernames[party] = [f.split('_2022')[0] for f in files]\n",
    "\n",
    "START_TIME = \"2022-10-16T00:00:00Z\"\n",
    "END_TIME = \"2023-10-15T23:59:59Z\"\n",
    "for party, people in people_before_elections.items():\n",
    "    for person in people:\n",
    "        if person in all_downloaded_usernames[party]:\n",
    "            print(f\"Skipping {person} from {party}, already downloaded.\")\n",
    "            continue\n",
    "        print(f\"Downloading tweets for {person} from {party} before elections.\")\n",
    "        download_tweets(person, party, True, START_TIME, END_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading tweets for all users after the elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping bartlomiejpejo from Konfederacja, already downloaded.\n",
      "Skipping GrzegorzBraun_ from Konfederacja, already downloaded.\n",
      "Skipping Iwaszkiewicz_RJ from Konfederacja, already downloaded.\n",
      "Skipping KonradBerkowicz from Konfederacja, already downloaded.\n",
      "Skipping MarSypniewski from Konfederacja, already downloaded.\n",
      "Skipping MichalWawer from Konfederacja, already downloaded.\n",
      "Skipping placzekgrzegorz from Konfederacja, already downloaded.\n",
      "Skipping SlawomirMentzen from Konfederacja, already downloaded.\n",
      "Skipping TudujKrzysztof from Konfederacja, already downloaded.\n",
      "Skipping Wlodek_Skalik from Konfederacja, already downloaded.\n",
      "Skipping WTumanowicz from Konfederacja, already downloaded.\n",
      "Skipping AndrzejSzejna from NL, already downloaded.\n",
      "Skipping AnitaKDZG from NL, already downloaded.\n",
      "Skipping DyduchMarek from NL, already downloaded.\n",
      "Skipping JoankaSW from NL, already downloaded.\n",
      "Skipping KGawkowski from NL, already downloaded.\n",
      "Skipping K_Smiszek from NL, already downloaded.\n",
      "Skipping MarcinKulasek from NL, already downloaded.\n",
      "Skipping MoskwaWodnicka from NL, already downloaded.\n",
      "Skipping PaulinaPW2024 from NL, already downloaded.\n",
      "Skipping poselTTrela from NL, already downloaded.\n",
      "Skipping RobertBiedron from NL, already downloaded.\n",
      "Skipping WandaNowicka from NL, already downloaded.\n",
      "Skipping wieczorekdarek from NL, already downloaded.\n",
      "Skipping wlodekczarzasty from NL, already downloaded.\n",
      "Skipping BeataSzydlo from PiS, already downloaded.\n",
      "Skipping elzbietawitek from PiS, already downloaded.\n",
      "Skipping Kaminski_M_ from PiS, already downloaded.\n",
      "Skipping Kowalczyk_H from PiS, already downloaded.\n",
      "Skipping Macierewicz_A from PiS, already downloaded.\n",
      "Skipping mblaszczak from PiS, already downloaded.\n",
      "Skipping MorawieckiM from PiS, already downloaded.\n",
      "Skipping mwojcik_ from PiS, already downloaded.\n",
      "Skipping PatrykJaki from PiS, already downloaded.\n",
      "Skipping AgaBaranowskaPL from PL2050, already downloaded.\n",
      "Skipping aga_buczynska from PL2050, already downloaded.\n",
      "Skipping hennigkloska from PL2050, already downloaded.\n",
      "Skipping joannamucha from PL2050, already downloaded.\n",
      "Skipping Kpelczynska from PL2050, already downloaded.\n",
      "Skipping LukaszOsmalak from PL2050, already downloaded.\n",
      "Skipping SlizPawel from PL2050, already downloaded.\n",
      "Skipping szymon_holownia from PL2050, already downloaded.\n",
      "Skipping ZalewskiPawel from PL2050, already downloaded.\n",
      "Skipping ZywnoMaciej from PL2050, already downloaded.\n",
      "Skipping bbudka from PO, already downloaded.\n",
      "Skipping CTomczyk from PO, already downloaded.\n",
      "Skipping donaldtusk from PO, already downloaded.\n",
      "Skipping DorotaNiedziela from PO, already downloaded.\n",
      "Skipping EwaKopacz from PO, already downloaded.\n",
      "Skipping JanGrabiec from PO, already downloaded.\n",
      "Skipping Konwinski_PO from PO, already downloaded.\n",
      "Skipping Leszczyna from PO, already downloaded.\n",
      "Skipping MKierwinski from PO, already downloaded.\n",
      "Skipping M_K_Blonska from PO, already downloaded.\n",
      "Skipping OklaDrewnowicz from PO, already downloaded.\n",
      "Skipping trzaskowski_ from PO, already downloaded.\n",
      "Skipping DariuszKlimczak from PSL, already downloaded.\n",
      "Skipping GrzybAndrzej from PSL, already downloaded.\n",
      "Skipping Hetman_K from PSL, already downloaded.\n",
      "Skipping JarubasAdam from PSL, already downloaded.\n",
      "Skipping KosiniakKamysz from PSL, already downloaded.\n",
      "Skipping Paslawska from PSL, already downloaded.\n",
      "Skipping PZgorzelskiP from PSL, already downloaded.\n",
      "Skipping StefanKrajewski from PSL, already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Read the people to download \n",
    "with open('../data/00.init/people_after_elections.json', 'r', encoding='utf-8') as f:\n",
    "    people_after_elections = json.load(f)\n",
    "\n",
    "# Check people which are already downloaded\n",
    "tweets_after_elections_path = \"../data/01.raw/tweets_after_elections\"\n",
    "all_files_after_elections = os.listdir(tweets_after_elections_path)\n",
    "all_files_dict = {}\n",
    "for party in all_files_after_elections:\n",
    "    party_folder = os.path.join(tweets_after_elections_path, party)\n",
    "    all_files_dict[party] = os.listdir(party_folder)\n",
    "all_downloaded_usernames = {}\n",
    "for party, files in all_files_dict.items():\n",
    "    all_downloaded_usernames[party] = [f.split('_2023')[0] for f in files]\n",
    "\n",
    "START_TIME = \"2023-10-16T00:00:00Z\"\n",
    "END_TIME = \"2024-10-15T23:59:59Z\"\n",
    "\n",
    "# Download tweets for all people after elections\n",
    "for party, people in people_after_elections.items():\n",
    "    for person in people:\n",
    "        if person in all_downloaded_usernames[party]:\n",
    "            print(f\"Skipping {person} from {party}, already downloaded.\")\n",
    "            continue\n",
    "        print(f\"Downloading tweets for {person} from {party} after elections.\")\n",
    "        download_tweets(person, party, False, START_TIME, END_TIME, client)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

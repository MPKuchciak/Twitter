{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to download tweets from the X API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os \n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK THE PATHS OF FILES !!! !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\Twitter\\\\notebooks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bear token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Bearer Token is required.\n",
      "Bearer Token successfully configured.\n"
     ]
    }
   ],
   "source": [
    "BEARER_TOKEN = input(\"Please enter your Bearer Token: \") \n",
    "\n",
    "if not BEARER_TOKEN:\n",
    "    print(\"Error: Bearer Token is required.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Bearer Token successfully configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_app_only(bearer_token):\n",
    "    \"\"\"\n",
    "    Authenticate with the Twitter API using Tweepy (App-Only Auth).\n",
    "    \"\"\"\n",
    "    return tweepy.Client(\n",
    "        bearer_token=bearer_token,\n",
    "        wait_on_rate_limit=True\n",
    "    )\n",
    "\n",
    "client = authenticate_app_only(BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function for feetching all tweet fields\n",
    "def fetch_all_tweet_fields(client, username, start_time, end_time, exclude_retweets=True, exclude_replies=True):\n",
    "    \"\"\"\n",
    "    Fetch a user's timeline with all possible fields and categorize tweets.\n",
    "    \"\"\"\n",
    "    # A) Get the user's ID\n",
    "    user_resp = client.get_user(username=username)\n",
    "    if not user_resp or not user_resp.data:\n",
    "        print(f\"ERROR: Could not find user '{username}'.\")\n",
    "        return []\n",
    "    user_id = user_resp.data.id\n",
    "\n",
    "    # B) Build exclude parameters\n",
    "    exclude_params = []\n",
    "    if exclude_retweets:\n",
    "        exclude_params.append(\"retweets\")\n",
    "    if exclude_replies:\n",
    "        exclude_params.append(\"replies\")\n",
    "\n",
    "    # C) Define fields and expansions\n",
    "    tweet_fields = [\n",
    "        \"attachments\", \"author_id\", \"context_annotations\", \"conversation_id\", \"created_at\",\n",
    "        \"edit_controls\", \"entities\", \"geo\", \"id\", \"in_reply_to_user_id\", \"lang\",\n",
    "        \"possibly_sensitive\", \"public_metrics\", \"referenced_tweets\", \"reply_settings\",\n",
    "        \"source\", \"text\", \"withheld\"\n",
    "    ]\n",
    "    user_fields = [\n",
    "        \"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"pinned_tweet_id\",\n",
    "        \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"withheld\"\n",
    "    ]\n",
    "    media_fields = [\n",
    "        \"duration_ms\", \"height\", \"media_key\", \"preview_image_url\", \"type\", \"url\",\n",
    "        \"width\", \"public_metrics\", \"alt_text\", \"variants\"\n",
    "    ]\n",
    "    place_fields = [\n",
    "        \"contained_within\", \"country\", \"country_code\", \"full_name\", \"geo\", \"id\", \"name\", \"place_type\"\n",
    "    ]\n",
    "    poll_fields = [\n",
    "        \"duration_minutes\", \"end_datetime\", \"id\", \"options\", \"voting_status\"\n",
    "    ]\n",
    "    expansions = [\n",
    "        \"attachments.poll_ids\", \"attachments.media_keys\", \"author_id\", \"in_reply_to_user_id\",\n",
    "        \"referenced_tweets.id\", \"referenced_tweets.id.author_id\", \"entities.mentions.username\",\n",
    "        \"geo.place_id\"\n",
    "    ]\n",
    "\n",
    "    # D) Fetch tweets using Tweepy Paginator\n",
    "    tweets = []\n",
    "    paginator = tweepy.Paginator(\n",
    "        client.get_users_tweets,\n",
    "        id=user_id,\n",
    "        tweet_fields=tweet_fields,\n",
    "        user_fields=user_fields,\n",
    "        media_fields=media_fields,\n",
    "        place_fields=place_fields,\n",
    "        poll_fields=poll_fields,\n",
    "        expansions=expansions,\n",
    "        exclude=exclude_params if exclude_params else None,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        max_results = 100  # Max per request                  \n",
    "    )\n",
    "    for tweet in paginator.flatten(limit=1000):\n",
    "        tweet_data = tweet.data\n",
    "        tweet_data[\"category\"] = categorize_tweet(tweet_data)  # Add categorization\n",
    "        tweets.append(tweet_data)\n",
    "    return tweets\n",
    "\n",
    "# Custom function for categorizing tweets\n",
    "def categorize_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Categorize the tweet as 'Original', 'Reply', 'Retweet', or 'Quote'.\n",
    "    \"\"\"\n",
    "    if \"referenced_tweets\" in tweet:\n",
    "        for ref in tweet[\"referenced_tweets\"]:\n",
    "            if ref[\"type\"] == \"retweeted\":\n",
    "                return \"Retweet\"\n",
    "            elif ref[\"type\"] == \"replied_to\":\n",
    "                return \"Reply\"\n",
    "            elif ref[\"type\"] == \"quoted\":\n",
    "                return \"Quote\"\n",
    "    elif \"in_reply_to_user_id\" in tweet and tweet[\"in_reply_to_user_id\"] is not None:\n",
    "        return \"Reply\"\n",
    "    return \"Original\"\n",
    "\n",
    "def save_tweets_to_json(tweets, filename, folder=\"../data/01.raw/tweets_data_final\"):\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tweets, f, ensure_ascii=False, indent=2, default=str)\n",
    "\n",
    "    print(f\"Saved {len(tweets)} tweets to {file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final downloading loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the final implementation for the Tweepy paginator. It does not resolve all issues, which is why we had to implement a custom while loop. In this loop, we check the date of the last downloaded tweet and request tweets between the `START_TIME` and the date of the last tweet ( + 1 second) to ensure we have downloaded all tweets from the specified time range. When some users post and retweet a lot, the API does not work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 667 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 781 tweets\n",
      "Fetching tweets from 2023-10-16T00:00:00Z to 2023-10-17T13:22:42Z\n",
      "Fetched 1 tweets\n",
      "Saved 782 tweets to tweets_data_final/PIS\\mwojcik__2023-10-16_2024-10-15.json.\n"
     ]
    }
   ],
   "source": [
    "username = \"mwojcik_\" #To change: Twitter username\n",
    "EXCLUDE_RETWEETS = True\n",
    "EXCLUDE_REPLIES = False\n",
    "\n",
    "START_TIME = \"2023-10-16T00:00:00Z\" #To change: Start time\n",
    "END_TIME = \"2024-10-15T23:59:59Z\" #To change: End time\n",
    "\n",
    "org_START_TIME = START_TIME\n",
    "org_END_TIME = END_TIME\n",
    "\n",
    "tweets = []\n",
    "\n",
    "new_tweets = fetch_all_tweet_fields(\n",
    "        client=client,\n",
    "        username=username,\n",
    "        start_time=START_TIME,\n",
    "        end_time=END_TIME,\n",
    "        exclude_retweets=EXCLUDE_RETWEETS,\n",
    "        exclude_replies=EXCLUDE_REPLIES\n",
    "    )\n",
    "tweets.extend(new_tweets)\n",
    "print(f'Fetched {len(new_tweets)} tweets')\n",
    "\n",
    "while (len(new_tweets) > 1):\n",
    "    last_tweet_time = datetime.fromisoformat(tweets[-1][\"created_at\"].replace(\"Z\", \"+00:00\"))\n",
    "    new_time = last_tweet_time + timedelta(seconds=1)\n",
    "    new_time_iso = new_time.isoformat().replace(\"+00:00\", \"Z\")\n",
    "    END_TIME = new_time_iso\n",
    "    print(f\"Fetching tweets from {START_TIME} to {END_TIME}\")\n",
    "    new_tweets = fetch_all_tweet_fields(\n",
    "        client=client,\n",
    "        username=username,\n",
    "        start_time=START_TIME,\n",
    "        end_time=END_TIME,\n",
    "        exclude_retweets=EXCLUDE_RETWEETS,\n",
    "        exclude_replies=EXCLUDE_REPLIES\n",
    "    )\n",
    "    print(f'Fetched {len(new_tweets)} tweets')\n",
    "    tweets.extend(new_tweets)    \n",
    "    sleep(10)\n",
    "\n",
    "save_tweets_to_json(tweets, f\"{username}_{org_START_TIME[0:10]}_{org_END_TIME[0:10]}.json\",folder=\"../data/01.raw/tweets_data_final/PiS\")  #To change: Folder !IMPORTANT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_twitter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\macie\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 20:41:33,659 - INFO - \n",
      "Fetching up to 5 tweets from @Arlukowicz...\n",
      "2025-01-09 20:41:34,169 - WARNING - Rate limit exceeded. Sleeping for 538 seconds.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Disclosure \n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# The `requests` library is open-source and free to use.\n",
    "# However, accessing Twitter's API may incur costs based on your API tier (Basic, Elevated, etc.).\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "BEARER_TOKEN = \"AAAAAAAAAAAAAAAAAAAAAIkYyAEAAAAAIh2Zu24EaVBRAmgAr5FoDZBdSgs%3DT62cDIf5iqmCk1Y4BfGC6aULnAcAJK3ps9AdSMM8ephmGaYpK7\"\n",
    "\n",
    "if not BEARER_TOKEN:\n",
    "    raise ValueError(\"Bearer Token not found. Please set TWITTER_BEARER_TOKEN in your environment variables.\")\n",
    "\n",
    "# List of politicians' usernames to fetch tweets from\n",
    "POLITICIANS = [\"Arlukowicz\"]  # Add more usernames as needed\n",
    "\n",
    "# Maximum tweets\n",
    "MAX_TWEETS = 5  # Adjust as needed (e.g., 3200)\n",
    "\n",
    "# Start & end times (ISO8601 format)\n",
    "START_TIME = \"2024-01-01T00:00:00Z\"\n",
    "END_TIME   = \"2024-12-31T23:59:59Z\"\n",
    "\n",
    "# Whether to exclude retweets and replies\n",
    "EXCLUDE_RETWEETS = True\n",
    "EXCLUDE_REPLIES  = True\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Logging Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"tweet_downloader.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Authentication Setup\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    \"\"\"\n",
    "    Create headers for Twitter API requests.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\",\n",
    "        \"User-Agent\": \"v2UserTweetsPython\"\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Categorize Tweets\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def categorize_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Categorize the tweet as 'Original', 'Reply', 'Retweet', or 'Quote'.\n",
    "    \"\"\"\n",
    "    if \"referenced_tweets\" in tweet:\n",
    "        for ref in tweet[\"referenced_tweets\"]:\n",
    "            if ref[\"type\"] == \"retweeted\":\n",
    "                return \"Retweet\"\n",
    "            elif ref[\"type\"] == \"replied_to\":\n",
    "                return \"Reply\"\n",
    "            elif ref[\"type\"] == \"quoted\":\n",
    "                return \"Quote\"\n",
    "    elif \"in_reply_to_user_id\" in tweet and tweet[\"in_reply_to_user_id\"] is not None:\n",
    "        return \"Reply\"\n",
    "    return \"Original\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5) Fetch Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def get_user_id(username, headers):\n",
    "    \"\"\"\n",
    "    Retrieves the user ID for a given username.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.twitter.com/2/users/by/username/{username}\"\n",
    "    params = {\n",
    "        \"user.fields\": \"id,name,username\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        logging.error(f\"Error fetching user ID for '{username}': {response.status_code} {response.text}\")\n",
    "        return None\n",
    "    \n",
    "    data = response.json()\n",
    "    if \"data\" in data:\n",
    "        return data[\"data\"][\"id\"]\n",
    "    else:\n",
    "        logging.warning(f\"No data found for user '{username}'.\")\n",
    "        return None\n",
    "\n",
    "def fetch_tweets(user_id, headers, max_tweets, start_time, end_time, exclude_retweets=True, exclude_replies=True):\n",
    "    \"\"\"\n",
    "    Fetches tweets for a given user ID with specified parameters.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.twitter.com/2/users/{user_id}/tweets\"\n",
    "    \n",
    "    # Define fields and expansions\n",
    "    tweet_fields = [\n",
    "        \"attachments\", \"author_id\", \"context_annotations\", \"conversation_id\", \"created_at\",\n",
    "        \"edit_controls\", \"entities\", \"geo\", \"id\", \"in_reply_to_user_id\", \"lang\",\n",
    "        \"possibly_sensitive\", \"public_metrics\", \"referenced_tweets\", \"reply_settings\",\n",
    "        \"source\", \"text\", \"withheld\"\n",
    "    ]\n",
    "    user_fields = [\n",
    "        \"created_at\", \"description\", \"entities\", \"id\", \"location\", \"name\", \"pinned_tweet_id\",\n",
    "        \"profile_image_url\", \"protected\", \"public_metrics\", \"url\", \"username\", \"verified\", \"withheld\"\n",
    "    ]\n",
    "    media_fields = [\n",
    "        \"duration_ms\", \"height\", \"media_key\", \"preview_image_url\", \"type\", \"url\",\n",
    "        \"width\", \"public_metrics\", \"alt_text\", \"variants\"\n",
    "    ]\n",
    "    place_fields = [\n",
    "        \"contained_within\", \"country\", \"country_code\", \"full_name\", \"geo\", \"id\", \"name\", \"place_type\"\n",
    "    ]\n",
    "    poll_fields = [\n",
    "        \"duration_minutes\", \"end_datetime\", \"id\", \"options\", \"voting_status\"\n",
    "    ]\n",
    "\n",
    "    # Define query parameters\n",
    "    params = {\n",
    "        \"max_results\": 5,  # Maximum allowed per request\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"tweet.fields\": \",\".join(tweet_fields),\n",
    "        \"user.fields\": \",\".join(user_fields),\n",
    "        \"media.fields\": \",\".join(media_fields),\n",
    "        \"place.fields\": \",\".join(place_fields),\n",
    "        \"poll.fields\": \",\".join(poll_fields),\n",
    "        \"expansions\": \",\".join([\n",
    "            \"attachments.poll_ids\",\n",
    "            \"attachments.media_keys\",\n",
    "            \"author_id\",\n",
    "            \"in_reply_to_user_id\",\n",
    "            \"referenced_tweets.id\",\n",
    "            \"referenced_tweets.id.author_id\",\n",
    "            \"entities.mentions.username\",\n",
    "            \"geo.place_id\",\n",
    "            \"edit_history_tweet_ids\"\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Handle exclusions\n",
    "    exclude = []\n",
    "    if exclude_retweets:\n",
    "        exclude.append(\"retweets\")\n",
    "    if exclude_replies:\n",
    "        exclude.append(\"replies\")\n",
    "    if exclude:\n",
    "        params[\"exclude\"] = \",\".join(exclude)\n",
    "    \n",
    "    tweets = []\n",
    "    next_token = None\n",
    "    max_iterations = 5  # Prevent infinite loops\n",
    "    iterations = 0\n",
    "    \n",
    "    while len(tweets) < max_tweets and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        if next_token:\n",
    "            params[\"pagination_token\"] = next_token\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 429:\n",
    "            # Rate limit exceeded\n",
    "            reset_time = int(response.headers.get(\"x-rate-limit-reset\", time.time() + 60))\n",
    "            sleep_duration = reset_time - int(time.time()) + 5  # Add buffer\n",
    "            if sleep_duration > 0:\n",
    "                logging.warning(f\"Rate limit exceeded. Sleeping for {sleep_duration} seconds.\")\n",
    "                time.sleep(sleep_duration)\n",
    "                continue  # Retry after sleeping\n",
    "            else:\n",
    "                # Reset time already passed\n",
    "                continue\n",
    "        elif response.status_code in {500, 502, 503, 504}:\n",
    "            # Handle server errors with exponential backoff\n",
    "            logging.error(f\"Server error {response.status_code}. Retrying after a short delay.\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        elif response.status_code == 401:\n",
    "            logging.error(\"Unauthorized. Check your Bearer Token.\")\n",
    "            break\n",
    "        elif response.status_code == 403:\n",
    "            logging.error(\"Forbidden. You might not have access to this resource.\")\n",
    "            break\n",
    "        elif response.status_code != 200:\n",
    "            logging.error(f\"Error fetching tweets: {response.status_code} {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        if \"data\" not in data:\n",
    "            logging.info(\"No more tweets found.\")\n",
    "            break\n",
    "        \n",
    "        fetched_tweets = data[\"data\"]\n",
    "        for tweet in fetched_tweets:\n",
    "            tweet[\"category\"] = categorize_tweet(tweet)\n",
    "        \n",
    "        tweets.extend(fetched_tweets)\n",
    "        logging.info(f\"Fetched {len(tweets)} tweets so far for user ID {user_id}.\")\n",
    "        \n",
    "        # Check for pagination\n",
    "        meta = data.get(\"meta\", {})\n",
    "        next_token = meta.get(\"next_token\", None)\n",
    "        if not next_token:\n",
    "            break  # No more pages\n",
    "        \n",
    "        # Respect rate limits by sleeping a bit between requests\n",
    "        time.sleep(1)  # Sleep 1 second between requests to be polite\n",
    "    \n",
    "    if iterations >= max_iterations:\n",
    "        logging.warning(\"Maximum iterations reached. There might be more tweets to fetch.\")\n",
    "    \n",
    "    # Trim to max_tweets if necessary\n",
    "    return tweets[:max_tweets]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6) Save Function\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def save_tweets_to_json(tweets, filename):\n",
    "    \"\"\"\n",
    "    Saves a list of tweet dictionaries to a JSON file, appending new tweets\n",
    "    and avoiding duplicates based on tweet IDs.\n",
    "    \"\"\"\n",
    "    existing_data = []\n",
    "    existing_ids = set()\n",
    "    \n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "                existing_data = json.load(f)\n",
    "                existing_ids = {tweet[\"id\"] for tweet in existing_data if \"id\" in tweet}\n",
    "        except json.JSONDecodeError:\n",
    "            logging.warning(f\"Warning: {filename} is not a valid JSON. Overwriting.\")\n",
    "            existing_data = []\n",
    "            existing_ids = set()\n",
    "    \n",
    "    # Filter out duplicate tweets\n",
    "    new_tweets = [tweet for tweet in tweets if tweet[\"id\"] not in existing_ids]\n",
    "    \n",
    "    if not new_tweets:\n",
    "        logging.info(f\"No new tweets to add for {filename}.\")\n",
    "        return\n",
    "    \n",
    "    # Append new tweets\n",
    "    existing_data.extend(new_tweets)\n",
    "    \n",
    "    # Save back to JSON\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logging.info(f\"Added {len(new_tweets)} new tweets to {filename}.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7) Main Execution\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    headers = create_headers(BEARER_TOKEN)\n",
    "    \n",
    "    for username in POLITICIANS:\n",
    "        logging.info(f\"\\nFetching up to {MAX_TWEETS} tweets from @{username}...\")\n",
    "        \n",
    "        user_id = get_user_id(username, headers)\n",
    "        if not user_id:\n",
    "            continue  # Skip to next user if ID not found\n",
    "        \n",
    "        tweets = fetch_tweets(\n",
    "            user_id=user_id,\n",
    "            headers=headers,\n",
    "            max_tweets=MAX_TWEETS,\n",
    "            start_time=START_TIME,\n",
    "            end_time=END_TIME,\n",
    "            exclude_retweets=EXCLUDE_RETWEETS,\n",
    "            exclude_replies=EXCLUDE_REPLIES\n",
    "        )\n",
    "        \n",
    "        if tweets:\n",
    "            filename = f\"{username}_tweets.json\"\n",
    "            save_tweets_to_json(tweets, filename)\n",
    "        else:\n",
    "            logging.info(f\"No tweets fetched for @{username}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
